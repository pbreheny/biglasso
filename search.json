[{"path":[]},{"path":"https://pbreheny.github.io/biglasso/articles/biglasso.html","id":"linear-regression","dir":"Articles","previous_headings":"Small Data","what":"Linear regression","title":"biglasso","text":"","code":"library(biglasso) #> Loading required package: bigmemory #> Loading required package: Matrix #> Loading required package: ncvreg  data(colon) X <- colon$X y <- colon$y dim(X) #> [1]   62 2000 X[1:5, 1:5] #>   Hsa.3004 Hsa.13491 Hsa.13491.1 Hsa.37254 Hsa.541 #> t  8589.42   5468.24     4263.41   4064.94 1997.89 #> n  9164.25   6719.53     4883.45   3718.16 2015.22 #> t  3825.71   6970.36     5369.97   4705.65 1166.55 #> n  6246.45   7823.53     5955.84   3975.56 2002.61 #> t  3230.33   3694.45     3400.74   3463.59 2181.42 ## convert X to a big.matrix object ## X.bm is a pointer to the data matrix X.bm <- as.big.matrix(X) str(X.bm) #> Formal class 'big.matrix' [package \"bigmemory\"] with 1 slot #>   ..@ address:<externalptr> dim(X.bm) #> [1]   62 2000 X.bm[1:5, 1:5] #>   Hsa.3004 Hsa.13491 Hsa.13491.1 Hsa.37254 Hsa.541 #> t  8589.42   5468.24     4263.41   4064.94 1997.89 #> n  9164.25   6719.53     4883.45   3718.16 2015.22 #> t  3825.71   6970.36     5369.97   4705.65 1166.55 #> n  6246.45   7823.53     5955.84   3975.56 2002.61 #> t  3230.33   3694.45     3400.74   3463.59 2181.42 ## same results as X[1:5, 1:5] ## fit entire solution path, using our newly proposed \"Adaptive\" screening rule (default) fit <- biglasso(X.bm, y) plot(fit)"},{"path":"https://pbreheny.github.io/biglasso/articles/biglasso.html","id":"cross-validation","dir":"Articles","previous_headings":"Small Data","what":"Cross-Validation","title":"biglasso","text":"cross-validation, things can : plot cross-validation plots:  Summarize CV object: Extract non-zero coefficients optimal λ\\lambda value:","code":"## 10-fold cross-valiation in parallel cvfit <- tryCatch(          {                 cv.biglasso(X.bm, y, seed = 1234, nfolds = 10, ncores = 4)          },          error = function(cond) {                 cv.biglasso(X.bm, y, seed = 1234, nfolds = 10, ncores = 2)          } ) par(mfrow = c(2, 2), mar = c(3.5, 3.5, 3, 1) ,mgp = c(2.5, 0.5, 0)) plot(cvfit, type = \"all\") summary(cvfit) #> lasso-penalized linear regression with n=62, p=2000 #> At minimum cross-validation error (lambda=0.0386): #> ------------------------------------------------- #>   Nonzero coefficients: 27 #>   Cross-validation error (deviance): 0.14 #>   R-squared: 0.40 #>   Signal-to-noise ratio: 0.66 #>   Scale estimate (sigma): 0.371 coef(cvfit)[which(coef(cvfit) != 0),] #>   (Intercept)      Hsa.8147     Hsa.43279     Hsa.36689      Hsa.3152  #>  6.882526e-01 -5.704059e-07 -2.748858e-08 -6.967419e-04  4.940698e-05  #>     Hsa.36665     Hsa.692.2      Hsa.1272       Hsa.166     Hsa.31801  #>  1.297733e-05 -1.878545e-04 -1.808689e-04  3.717512e-04  1.119437e-04  #>      Hsa.3648      Hsa.1047     Hsa.13628      Hsa.3016      Hsa.5392  #>  1.508691e-04  6.557284e-07  6.519466e-05  2.479566e-05  5.741251e-04  #>      Hsa.1832      Hsa.1464     Hsa.12241     Hsa.44244      Hsa.9246  #> -4.052627e-05  1.821951e-05 -1.912212e-04 -3.369856e-04 -1.582765e-06  #>     Hsa.41159     Hsa.33268      Hsa.6814      Hsa.1660       Hsa.404  #>  3.974870e-04 -4.911208e-04  5.639023e-04  5.171245e-04 -5.208537e-05  #>     Hsa.43331      Hsa.1491   Hsa.41098.1  #> -6.853944e-04  2.977285e-04 -1.748628e-04"},{"path":"https://pbreheny.github.io/biglasso/articles/biglasso.html","id":"logistic-regression","dir":"Articles","previous_headings":"Small Data","what":"Logistic Regression","title":"biglasso","text":"","code":"data(Heart) X <- Heart$X y <- Heart$y X.bm <- as.big.matrix(X) fit <- biglasso(X.bm, y, family = \"binomial\") plot(fit)"},{"path":"https://pbreheny.github.io/biglasso/articles/biglasso.html","id":"cox-regression","dir":"Articles","previous_headings":"Small Data","what":"Cox Regression","title":"biglasso","text":"","code":"library(survival) #>  #> Attaching package: 'survival' #> The following object is masked _by_ '.GlobalEnv': #>  #>     colon X <- heart[,4:7] y <- Surv(heart$stop - heart$start, heart$event) X.bm <- as.big.matrix(X) #> Warning in as.big.matrix(X): Coercing data.frame to matrix via factor level #> numberings. fit <- biglasso(X.bm, y, family = \"cox\") plot(fit)"},{"path":"https://pbreheny.github.io/biglasso/articles/biglasso.html","id":"multiple-response-linear-regression-multi-task-learning","dir":"Articles","previous_headings":"Small Data","what":"Multiple response Linear Regression (multi-task learning)","title":"biglasso","text":"","code":"set.seed(10101) n=300; p=300; m=5; s=10; b=1 x = matrix(rnorm(n * p), n, p) beta = matrix(seq(from=-b,to=b,length.out=s*m),s,m) y = x[,1:s] %*% beta + matrix(rnorm(n*m,0,1),n,m) x.bm = as.big.matrix(x) fit = biglasso(x.bm, y, family = \"mgaussian\") plot(fit)"},{"path":"https://pbreheny.github.io/biglasso/articles/biglasso.html","id":"big-data","dir":"Articles","previous_headings":"","what":"Big Data","title":"biglasso","text":"raw data file large, ’s better convert raw data file file-backed big.matrix using file cache. can call function setupX, reads raw data file creates backing file (.bin) descriptor file (.desc) raw data matrix: ’s important note operation just one-time execution. done, data can always retrieved seamlessly attaching descriptor file (.desc) new R session: appealing big data analysis don’t need “read” raw data R session, time-consuming. code fits lasso-penalized linear model, runs 10-fold cross-validation:","code":"## The data has 200 observations, 600 features, and 10 non-zero coefficients. ## This is not actually very big, but vignettes in R are supposed to render ## quickly. Much larger data can be handled in the same way. if(!file.exists('BigX.bin')) {   X <- matrix(rnorm(1000 * 5000), 1000, 5000)   beta <- c(-5:5)   y <- as.numeric(X[,1:11] %*% beta)   write.csv(X, \"BigX.csv\", row.names = F)   write.csv(y, \"y.csv\", row.names = F)   ## Pretend that the data in \"BigX.csv\" is too large to fit into memory   X.bm <- setupX(\"BigX.csv\", header = T) } #> Reading data from file, and creating file-backed big.matrix... #> This should take a while if the data is very large... #> Start time:  2025-03-05 17:44:34  #> End time:  2025-03-05 17:44:36  #> DONE! #>  #> Note: This function needs to be called only one time to create two backing #>       files (.bin, .desc) in current dir. Once done, the data can be #>       'loaded' using function 'attach.big.matrix'. See details in doc. rm(list = c(\"X\", \"X.bm\", \"y\")) # Pretend starting a new session X.bm <- attach.big.matrix(\"BigX.desc\") y <- read.csv(\"y.csv\")[,1] system.time({fit <- biglasso(X.bm, y)}) #>    user  system elapsed  #>   0.269   0.001   0.271 plot(fit) # 10-fold cross validation in parallel tryCatch(     {         system.time({cvfit <- cv.biglasso(X.bm, y, seed = 1234, ncores = 4, nfolds = 10)})     },     error = function(cond) {         system.time({cvfit <- cv.biglasso(X.bm, y, seed = 1234, ncores = 2, nfolds = 10)})     } ) #>    user  system elapsed  #>   0.411   0.005   2.922 par(mfrow = c(2, 2), mar = c(3.5, 3.5, 3, 1), mgp = c(2.5, 0.5, 0)) plot(cvfit, type = \"all\")"},{"path":"https://pbreheny.github.io/biglasso/articles/biglasso.html","id":"links","dir":"Articles","previous_headings":"","what":"Links","title":"biglasso","text":"biglasso CRAN biglasso GitHub biglasso website big.matrix manipulation","code":""},{"path":"https://pbreheny.github.io/biglasso/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Yaohui Zeng. Author. Chuyi Wang. Author. Tabitha Peter. Author. Patrick Breheny. Author, maintainer.","code":""},{"path":"https://pbreheny.github.io/biglasso/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Zeng Y, Breheny P (2021). “biglasso Package: Memory- Computation-Efficient Solver Lasso Model Fitting Big Data R.” R Journal, 12(2), 6–19. doi:10.32614/RJ-2021-001, 1701.05936, https://doi.org/10.32614/RJ-2021-001.","code":"@Article{,   author = {Yaohui Zeng and Patrick Breheny},   title = {The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R},   journal = {R Journal},   eprint = {1701.05936},   year = {2021},   number = {2},   pages = {6--19},   volume = {12},   doi = {10.32614/RJ-2021-001},   url = {https://doi.org/10.32614/RJ-2021-001}, }"},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"biglasso-extend-lasso-model-fitting-to-big-data-in-r","dir":"","previous_headings":"","what":"Extending Lasso Model Fitting to Big Data","title":"Extending Lasso Model Fitting to Big Data","text":"biglasso extends lasso elastic-net linear logistic regression models ultrahigh-dimensional, multi-gigabyte data sets loaded memory. utilizes memory-mapped files store massive data disk read memory whenever necessary model fitting. Moreover, advanced feature screening rules proposed implemented accelerate model fitting. result, package much memory- computation-efficient highly scalable compared existing lasso-fitting packages glmnet ncvreg. Bechmarking experiments using simulated real data sets show biglasso 1.5x 4x times faster existing packages, also least 2x memory-efficient. importantly, best knowledge, biglasso first R package enables users fit lasso models data sets larger available RAM, thus allowing powerful big data analysis ordinary laptop.","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Extending Lasso Model Fitting to Big Data","text":"install latest stable release version CRAN: install latest development version GitHub:","code":"install.packages(\"biglasso\") remotes::install_github(\"pbreheny/biglasso\")"},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"news","dir":"","previous_headings":"","what":"News","title":"Extending Lasso Model Fitting to Big Data","text":"See NEWS.md latest news. technical paper package selected Winner 2017 ASA Student Paper Competiton Section Statistical Computing. package finished top 3 2017 ASA Chambers Statistical Software Award.","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Extending Lasso Model Fitting to Big Data","text":"R Reference manual Package Website technical papers package: ) software paper; ii) paper hybrid safe-strong rules","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Extending Lasso Model Fitting to Big Data","text":"utilizes memory-mapped files store massive data disk, loading data memory necessary model fitting. Consequently, ’s able seamlessly handle --core computation. built upon pathwise coordinate descent algorithm warm start, active set cycling, feature screening strategies, proven one fastest lasso solvers. develop new, adaptive feature screening rules outperform state---art screening rules sequential strong rule (SSR) sequential EDPP rule (SEDPP) additional 1.5x 4x speedup. implementation designed memory-efficient possible eliminating extra copies data created R packages, making biglasso least 2x memory-efficient glmnet. underlying computation implemented C++, parallel computing OpenMP also supported.","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"simulated-data","dir":"","previous_headings":"Benchmarks:","what":"Simulated data:","title":"Extending Lasso Model Fitting to Big Data","text":"Packages compared: biglasso (1.4-0), glmnet (4.0-2), ncvreg (3.12-0), picasso (1.3-1). Platform: AMD Ryzen 5 5600X @ 4.2 GHz 32 GB RAM. Experiments: solving lasso-penalized linear regression entire path 100 lambda values equally spaced log scale lambda / lambda_max 0.1 1; varying number observations n number features p; 20 replications, mean computing time (seconds) reported. Data generating model: y =  X *  beta + 0.1 eps, X eps ..d. sampled N(0, 1).","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"id_1-biglasso-is-more-computation-efficient","dir":"","previous_headings":"Benchmarks: > Simulated data:","what":"(1) biglasso is more computation-efficient:","title":"Extending Lasso Model Fitting to Big Data","text":"settings, biglasso (1 core) uniformly faster picasso, glmnet ncvreg. data gets bigger, biglasso achieves 6-9x speed-compared packages. Moreover, computing time biglasso can reduced half via parallel-computation multiple cores.","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"id_2-biglasso-is-more-memory-efficient","dir":"","previous_headings":"Benchmarks: > Simulated data:","what":"(2) biglasso is more memory-efficient:","title":"Extending Lasso Model Fitting to Big Data","text":"prove biglasso much memory-efficient, simulate 1000 X 100000 large feature matrix. raw data 0.75 GB. used Syrupy measure memory used RAM (.e. resident set size, RSS) every 1 second lasso model fitting packages. maximum RSS (GB) used single fit 10-fold cross validation reported Table . single fit case, biglasso consumes 0.60 GB memory RAM, 23% used glmnet 24% used ncvreg. Note memory consumed glmnet ncvreg respectively 3.4x 3.3x larger size raw data. biglasso also requires less additional memory perform cross-validation, compared packages. serial 10-fold cross-validation, biglasso requires just 31% memory used glmnet 11% used ncvreg, making 3.2x 9.4x memory-efficient compared two, respectively. Note: ..* memory savings offered biglasso even significant cross-validation conducted parallel. However, measuring memory usage across parallel processes straightforward implemented Syrupy; ..* cross-validation implemented picasso point.","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"real-data","dir":"","previous_headings":"Benchmarks:","what":"Real data:","title":"Extending Lasso Model Fitting to Big Data","text":"performance packages also tested using diverse real data sets: * Breast cancer gene expression data (GENE); * MNIST handwritten image data (MNIST); * Cardiac fibrosis genome-wide association study data (GWAS); * Subset New York Times bag--words data (NYT). following table summarizes mean (SE) computing time (seconds) solving lasso along entire path 100 lambda values equally spaced log scale lambda / lambda_max 0.1 1 20 replications.","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"big-data-out-of-core-computation","dir":"","previous_headings":"Benchmarks:","what":"Big data: Out-of-core computation","title":"Extending Lasso Model Fitting to Big Data","text":"demonstrate --core computing capability biglasso, 96 GB real data set large-scale genome-wide association study analyzed. dimensionality design matrix : n = 973, p = 11,830,470. Note size data 3x larger installed 32 GB RAM. Since three packages handle data-larger--RAM case, compare performance screening rules SSR Adaptive based package biglasso. addition, two cases terms lambda_min considered: (1) lam_min = 0.1 lam_max; (2) lam_min = 0.5 lam_max, practice typically less interest lower values lambdafor high-dimensional data case. entire solution path 100 lambda values obtained. table summarizes overall computing time (minutes) screening rule SSR (three packages using) new rule Adaptive. (replication conducted.)","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"reference","dir":"","previous_headings":"","what":"Reference:","title":"Extending Lasso Model Fitting to Big Data","text":"Zeng Y Breheny P (2021). biglasso Package: Memory- Computation-Efficient Solver Lasso Model Fitting Big Data R. R Journal, 12: 6-19. URL https://doi.org/10.32614/RJ-2021-001 Zeng Y, Yang T, Breheny P (2021). Hybrid safe-strong rules efficient optimization lasso-type problems. Computational Statistics Data Analysis, 153: 107063. URL https://doi.org/10.1016/j.csda.2020.107063 Wang C Breheny P (2022). Adaptive hybrid screening efficient lasso optimization. Journal Statistical Computation Simulation, 92: 2233–2256. URL https://doi.org/10.1080/00949655.2021.2025376 Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., Tibshirani, R. J. (2012). Strong rules discarding predictors lasso-type problems. Journal Royal Statistical Society: Series B (Statistical Methodology), 74 (2), 245-266. Wang, J., Zhou, J., Wonka, P., Ye, J. (2013). Lasso screening rules via dual polytope projection. Advances Neural Information Processing Systems, pp. 1070-1078. Xiang, Z. J., Ramadge, P. J. (2012, March). Fast lasso screening tests based correlations. Acoustics, Speech Signal Processing (ICASSP), 2012 IEEE International Conference (pp. 2137-2140). IEEE. Wang, J., Zhou, J., Liu, J., Wonka, P., Ye, J. (2014). safe screening rule sparse logistic regression. Advances Neural Information Processing Systems, pp. 1053-1061.","code":""},{"path":"https://pbreheny.github.io/biglasso/index.html","id":"report-bugs","dir":"","previous_headings":"","what":"Report bugs：","title":"Extending Lasso Model Fitting to Big Data","text":"open issue send email Patrick Breheny patrick-breheny@uiowa.edu","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Extending Lasso Model Fitting to Big Data — biglasso-package","title":"Extending Lasso Model Fitting to Big Data — biglasso-package","text":"Extend lasso elastic-net linear, logistic cox regression models ultrahigh-dimensional, multi-gigabyte data sets loaded available RAM. package utilizes memory-mapped files store massive data disk read memory whenever necessary model fitting. Moreover, advanced feature screening rules proposed implemented accelerate model fitting. result, package much memory- computation-efficient highly scalable compared existing lasso-fitting packages glmnet ncvreg, thus allowing powerful big data analysis even ordinary laptop.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extending Lasso Model Fitting to Big Data — biglasso-package","text":"Penalized regression models, particular lasso, extensively applied analyzing high-dimensional data sets. However, due memory limit, existing R packages capable fitting lasso models ultrahigh-dimensional, multi-gigabyte data sets increasingly seen many areas genetics, biomedical imaging, genome sequencing high-frequency finance. package aims fill gap extending lasso model fitting Big Data R. Version >= 1.2-3 represents major redesign source code converted C++ (previously C), new feature screening rules, well OpenMP parallel computing, implemented. key features biglasso summarized : utilizes memory-mapped files store massive data disk, loading data memory necessary model fitting. Consequently, able seamlessly data-larger--RAM cases. built upon pathwise coordinate descent algorithm warm start, active set cycling, feature screening strategies, proven one fastest lasso solvers. incorporates newly developed hybrid adaptive screening outperform state---art screening rules sequential strong rule (SSR) sequential EDPP rule (SEDPP) additional 1.5x 4x speedup. implementation designed memory-efficient possible eliminating extra copies data created R packages, making least 2x memory-efficient glmnet. underlying computation implemented C++, parallel computing OpenMP also supported. information: Benchmarking results: https://github.com/pbreheny/biglasso Tutorial: https://pbreheny.github.io/biglasso/articles/biglasso.html Technical paper: https://arxiv.org/abs/1701.05936","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso-package.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Extending Lasso Model Fitting to Big Data — biglasso-package","text":"input design matrix X must bigmemory::big.matrix() object. can created function .big.matrix R package bigmemory. data (design matrix) large (e.g. 10 GB) stored external file, often case big data, X can created calling function setupX(). case, several restrictions data file: data file must well-formated ASCII-file, row corresponding observation column variable; data file must contain one single type. Current version supports double type; data file must contain numeric variables. categorical variables, user needs create dummy variables categorical varable (adding additional columns). Future versions try address restrictions. Denote number observations variables , respectively, n p. worth noting package suitable wide data (ultrahigh-dimensional, p >> n) compared long data (n >> p). model fitting algorithm takes advantage sparsity assumption high-dimensional data. just give user ideas, benchmarking results total computing time (seconds) solving lasso-penalized linear regression along sequence 100 values tuning parameter. cases, assume 20 non-zero coefficients equal +/- 2 true model. (Based Version 1.2-3, screening rule \"SSR-BEDPP\" used) wide data case (p > n), n = 1,000: % long data case (n >> p), p = 1,000: %","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Extending Lasso Model Fitting to Big Data — biglasso-package","text":"Zeng Y Breheny P. (2021) biglasso Package: Memory- Computation-Efficient Solver Lasso Model Fitting Big Data R. R Journal, 12: 6-19. doi:10.32614/RJ-2021-001 Wang C Breheny P. (2022) Adaptive hybrid screening efficient lasso optimization. Journal Statistical Computation Simulation, 92: 2233-2256. doi:10.1080/00949655.2021.2025376 Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., Tibshirani, R. J. (2012). Strong rules discarding predictors lasso-type problems. Journal Royal Statistical Society: Series B (Statistical Methodology), 74(2), 245-266. Wang, J., Zhou, J., Wonka, P., Ye, J. (2013). Lasso screening rules via dual polytope projection. Advances Neural Information Processing Systems, pp. 1070-1078. Xiang, Z. J., Ramadge, P. J. (2012). Fast lasso screening tests based correlations. Acoustics, Speech Signal Processing (ICASSP), 2012 IEEE International Conference (pp. 2137-2140). IEEE. Wang, J., Zhou, J., Liu, J., Wonka, P., Ye, J. (2014). safe screening rule sparse logistic regression. Advances Neural Information Processing Systems, pp. 1053-1061.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extending Lasso Model Fitting to Big Data — biglasso-package","text":"Yaohui Zeng, Chuyi Wang, Tabitha Peter, Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extending Lasso Model Fitting to Big Data — biglasso-package","text":"","code":"if (FALSE) { # \\dontrun{ ## Example of reading data from external big data file, fit lasso model,  ## and run cross validation in parallel  # simulated design matrix, 1000 observations, 500,000 variables, ~ 5GB # there are 10 true variables with non-zero coefficient 2. xfname <- 'x_e3_5e5.txt'  yfname <- 'y_e3_5e5.txt' # response vector time <- system.time(   X <- setupX(xfname, sep = '\\t') # create backing files (.bin, .desc) ) print(time) # ~ 7 minutes; this is just one-time operation dim(X)  # the big.matrix then can be retrieved by its descriptor file (.desc) in any new R session.  rm(X) xdesc <- 'x_e3_5e5.desc'  X <- attach.big.matrix(xdesc) dim(X)  y <- as.matrix(read.table(yfname, header = F)) time.fit <- system.time(   fit <- biglasso(X, y, family = 'gaussian', screen = 'Hybrid') ) print(time.fit) # ~ 44 seconds for fitting a lasso model along the entire solution path  # cross validation in parallel seed <- 1234 time.cvfit <- system.time(   cvfit <- cv.biglasso(X, y, family = 'gaussian', screen = 'Hybrid',                         seed = seed, ncores = 4, nfolds = 10) ) print(time.cvfit) # ~ 3 minutes for 10-fold cross validation plot(cvfit) summary(cvfit) } # }"},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit lasso penalized regression path for big data — biglasso","title":"Fit lasso penalized regression path for big data — biglasso","text":"Extend lasso model fitting big data loaded memory. Fit solution paths linear, logistic Cox regression models penalized lasso, ridge, elastic-net grid values regularization parameter lambda.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit lasso penalized regression path for big data — biglasso","text":"","code":"biglasso(   X,   y,   row.idx = 1:nrow(X),   penalty = c(\"lasso\", \"ridge\", \"enet\"),   family = c(\"gaussian\", \"binomial\", \"cox\", \"mgaussian\"),   alg.logistic = c(\"Newton\", \"MM\"),   screen = c(\"Adaptive\", \"SSR\", \"Hybrid\", \"None\"),   safe.thresh = 0,   update.thresh = 1,   ncores = 1,   alpha = 1,   lambda.min = ifelse(nrow(X) > ncol(X), 0.001, 0.05),   nlambda = 100,   lambda.log.scale = TRUE,   lambda,   eps = 1e-07,   max.iter = 1000,   dfmax = ncol(X) + 1,   penalty.factor = rep(1, ncol(X)),   warn = TRUE,   output.time = FALSE,   return.time = TRUE,   verbose = FALSE )"},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit lasso penalized regression path for big data — biglasso","text":"X design matrix, without intercept. must double type bigmemory::big.matrix() object. function standardizes data includes intercept internally default model fitting. y response vector family=\"gaussian\" family=\"binomial\". family=\"cox\", y two-column matrix columns 'time' 'status'. latter binary variable, '1' indicating death, '0' indicating right censored. family=\"mgaussin\", y n*m matrix n sample size m number responses. row.idx integer vector row indices X used fitting model. 1:nrow(X) default. penalty penalty applied model. Either \"lasso\" (default), \"ridge\", \"enet\" (elastic net). family Either \"gaussian\", \"binomial\", \"cox\" \"mgaussian\" depending response. alg.logistic algorithm used logistic regression. \"Newton\" exact hessian used (default); \"MM\" majorization-minimization algorithm used set upper-bound hessian matrix. can faster, particularly data-larger--RAM case. screen feature screening rule used lambda discards features speed computation: \"SSR\" (default penalty=\"ridge\" penalty=\"enet\" )sequential strong rule; \"Hybrid\" newly proposed hybrid screening rules combine strong rule safe rule. \"Adaptive\" (default penalty=\"lasso\" without penalty.factor) newly proposed adaptive rules reuse screening reference multiple lambda values. Note : (1) linear regression elastic net penalty, \"SSR\" \"Hybrid\" applicable since version 1.3-0;  (2) \"SSR\" applicable elastic-net-penalized logistic regression cox regression; (3) active set cycling strategy incorporated screening rules. safe.thresh threshold value 0 1 controls stop safe test. example, 0.01 means stop safe test next lambda iteration number features rejected safe test current lambda iteration larger 1\\ always turn safe test, whereas 0 (default) means turn safe test number features rejected safe test 0 current lambda. update.thresh non negative threshold value controls often update reference safe rules \"Adaptive\" methods. Smaller value means updating often. ncores number OpenMP threads used parallel computing. alpha elastic-net mixing parameter controls relative contribution lasso (l1) ridge (l2) penalty. penalty defined $$ \\alpha||\\beta||_1 + (1-\\alpha)/2||\\beta||_2^2.$$ alpha=1 lasso penalty, alpha=0 ridge penalty, alpha 0 1 elastic-net (\"enet\") penalty. lambda.min smallest value lambda, fraction lambda.max.  Default .001 number observations larger number covariates .05 otherwise. nlambda number lambda values.  Default 100. lambda.log.scale Whether compute grid values lambda log scale (default) linear scale. lambda user-specified sequence lambda values.  default, sequence values length nlambda computed, equally spaced log scale. eps Convergence threshold inner coordinate descent.  algorithm iterates maximum change objective coefficient update less eps times null deviance. Default value 1e-7. max.iter Maximum number iterations.  Default 1000. dfmax Upper bound number nonzero coefficients.  Default upper bound.  However, large data sets, computational burden may heavy models large number nonzero coefficients. penalty.factor multiplicative factor penalty applied coefficient. supplied, penalty.factor must numeric vector length equal number columns X.  purpose penalty.factor apply differential penalization coefficients thought likely others model. Current package allow unpenalized coefficients. ispenalty.factor 0. penalty.factor supported \"SSR\" screen. warn Return warning messages failures converge model saturation?  Default TRUE. output.time Whether print start end time model fitting. Default FALSE. return.time Whether return computing time model fitting. Default TRUE. verbose Whether output timing lambda iteration. Default FALSE.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit lasso penalized regression path for big data — biglasso","text":"object S3 class \"biglasso\" \"gaussian\", \"binomial\", \"cox\" families, object S3 class \"mbiglasso\" \"mgaussian\" family,  following variables. beta fitted matrix coefficients, store sparse matrix representation. number rows equal number coefficients, whereas number columns equal nlambda. \"mgaussian\" family m responses, list m matrices. iter vector length nlambda containing number iterations convergence value lambda. lambda sequence regularization parameter values path. penalty . family . alpha . loss vector containing either residual sum squares (\"gaussian\", \"mgaussian\") negative log-likelihood (\"binomial\", \"cox\") fitted model value lambda. penalty.factor . n number observations used model fitting. equal length(row.idx). center sample mean vector variables, .e., column mean sub-matrix X used model fitting. scale sample standard deviation variables, .e., column standard deviation sub-matrix X used model fitting. y response vector used model fitting. Depending row.idx, subset raw input response vector y. screen . col.idx indices features 'scale' value greater 1e-6. Features 'scale' less 1e-6 removed model fitting. rejections number features rejected value lambda. safe_rejections number features rejected safe rules value lambda.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit lasso penalized regression path for big data — biglasso","text":"objective function linear regression multiple responses linear regression (family = \"gaussian\" family = \"mgaussian\") $$\\frac{1}{2n}\\textrm{RSS} + \\lambda*\\textrm{penalty},$$ family = \"mgaussian\"), group-lasso type penalty applied. logistic regression (family = \"binomial\") $$-\\frac{1}{n} loglike + \\lambda*\\textrm{penalty},$$, cox regression, breslow approximation ties applied. Several advanced feature screening rules implemented. lasso-penalized linear regression, options screen applicable. proposal adaptive rule - \"Adaptive\" - achieves highest speedup recommended one, especially ultrahigh-dimensional large-scale data sets. cox regression /elastic net penalty, \"SSR\" applicable now. efficient rules development.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit lasso penalized regression path for big data — biglasso","text":"Zeng Y Breheny P. (2021) biglasso Package: Memory- Computation- Efficient Solver Lasso Model Fitting Big Data R. R Journal, 12: 6-19. doi:10.32614/RJ-2021-001","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit lasso penalized regression path for big data — biglasso","text":"Yaohui Zeng, Chuyi Wang Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit lasso penalized regression path for big data — biglasso","text":"","code":"## Linear regression data(colon) X <- colon$X y <- colon$y X.bm <- as.big.matrix(X) # lasso, default par(mfrow=c(1,2)) fit.lasso <- biglasso(X.bm, y, family = 'gaussian') plot(fit.lasso, log.l = TRUE, main = 'lasso') # elastic net fit.enet <- biglasso(X.bm, y, penalty = 'enet', alpha = 0.5, family = 'gaussian') plot(fit.enet, log.l = TRUE, main = 'elastic net, alpha = 0.5')   ## Logistic regression data(colon) X <- colon$X y <- colon$y X.bm <- as.big.matrix(X) # lasso, default par(mfrow = c(1, 2)) fit.bin.lasso <- biglasso(X.bm, y, penalty = 'lasso', family = \"binomial\") plot(fit.bin.lasso, log.l = TRUE, main = 'lasso') # elastic net fit.bin.enet <- biglasso(X.bm, y, penalty = 'enet', alpha = 0.5, family = \"binomial\") plot(fit.bin.enet, log.l = TRUE, main = 'elastic net, alpha = 0.5')   ## Cox regression set.seed(10101) N <- 1000; p <- 30; nzc <- p/3 X <- matrix(rnorm(N * p), N, p) beta <- rnorm(nzc) fx <- X[, seq(nzc)] %*% beta/3 hx <- exp(fx) ty <- rexp(N, hx) tcens <- rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator y <- cbind(time = ty, status = 1 - tcens)  # y <- Surv(ty, 1 - tcens) with library(survival) X.bm <- as.big.matrix(X) fit <- biglasso(X.bm, y, family = \"cox\") plot(fit, main = \"cox\")  ## Multiple responses linear regression set.seed(10101) n=300; p=300; m=5; s=10; b=1 x = matrix(rnorm(n * p), n, p) beta = matrix(seq(from=-b,to=b,length.out=s*m),s,m) y = x[,1:s] %*% beta + matrix(rnorm(n*m,0,1),n,m) x.bm = as.big.matrix(x) fit = biglasso(x.bm, y, family = \"mgaussian\") plot(fit, main = \"mgaussian\")"},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","title":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","text":"function intended users know exactly want complete control fitting process. add intercept standardize design matrix set path lambda (lasso tuning parameter) critical steps data analysis. However, direct API provided use situations lasso fitting process internal component complicated algorithm standardization must handled externally.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","text":"","code":"biglasso_fit(   X,   y,   r,   init = rep(0, ncol(X)),   xtx,   penalty = \"lasso\",   lambda,   alpha = 1,   gamma,   ncores = 1,   max.iter = 1000,   eps = 1e-05,   dfmax = ncol(X) + 1,   penalty.factor = rep(1, ncol(X)),   warn = TRUE,   output.time = FALSE,   return.time = TRUE )"},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","text":"X design matrix, without intercept. must double type bigmemory::big.matrix() object. y response vector r Residuals (length n vector) corresponding init. WARNING: supply incorrect value r, solution incorrect. init Initial values beta.  Default: zero (length p vector) xtx X scales: jth element equal crossprod(X[,j])/n. particular, X standardized, one pass xtx = rep(1, p).  WARNING: supply incorrect value xtx, solution incorrect. (length p vector) penalty String specifying penalty use. Default 'lasso', options 'SCAD' 'MCP' (latter non-convex) lambda single value lasso tuning parameter. alpha elastic-net mixing parameter controls relative contribution lasso (l1) ridge (l2) penalty. penalty defined : $$ \\alpha||\\beta||_1 + (1-\\alpha)/2||\\beta||_2^2.$$ alpha=1 lasso penalty, alpha=0 ridge penalty, alpha 0 1 elastic-net (\"enet\") penalty. gamma Tuning parameter value nonconvex penalty. Defaults 3.7 penalty = 'SCAD' 3 penalty = 'MCP' ncores number OpenMP threads used parallel computing. max.iter Maximum number iterations.  Default 1000. eps Convergence threshold inner coordinate descent. algorithm iterates maximum change objective coefficient update less eps times null deviance. Default value 1e-7. dfmax Upper bound number nonzero coefficients. Default upper bound.  However, large data sets, computational burden may heavy models large number nonzero coefficients. penalty.factor multiplicative factor penalty applied coefficient. supplied, penalty.factor must numeric vector length equal number columns X. warn Return warning messages failures converge model saturation?  Default TRUE. output.time Whether print start end time model fitting. Default FALSE. return.time Whether return computing time model fitting. Default TRUE.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","text":"object S3 class \"biglasso\" following variables. beta vector estimated coefficients iter vector length nlambda containing number iterations convergence resid Vector residuals calculated estimated coefficients. lambda sequence regularization parameter values path. alpha biglasso() loss vector containing either residual sum squares fitted model value lambda. penalty.factor biglasso(). n number observations used model fitting. y response vector used model fitting.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","text":"Note: Hybrid safe-strong rules turned biglasso_fit(), rely standardization Currently, function works linear regression (family = 'gaussian').","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_fit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","text":"Tabitha Peter Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Direct interface to biglasso fitting, no preprocessing — biglasso_fit","text":"","code":"data(Prostate) X <- cbind(1, Prostate$X) xtx <- apply(X, 2, crossprod)/nrow(X) y <- Prostate$y X.bm <- as.big.matrix(X) init <- rep(0, ncol(X)) fit <- biglasso_fit(X = X.bm, y = y, r=y, init = init, xtx = xtx,   lambda = 0.1, penalty.factor=c(0, rep(1, ncol(X)-1)), max.iter = 10000) fit$beta #>                    lcavol      lweight          age         lbph          svi  #>  1.725303940  0.577848489  0.043409725 -0.005572137  0.076326241  0.000000000  #>          lcp      gleason        pgg45  #>  0.000000000  0.000000000  0.006712771     fit <- biglasso_fit(X = X.bm, y = y, r=y, init = init, xtx = xtx, penalty='MCP',   lambda = 0.1, penalty.factor=c(0, rep(1, ncol(X)-1)), max.iter = 10000) fit$beta #>                    lcavol      lweight          age         lbph          svi  #>  2.268444208  0.677388754  0.000000000 -0.013317940  0.143711214  0.000000000  #>          lcp      gleason        pgg45  #>  0.000000000  0.000000000  0.005398707"},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","title":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","text":"function intended users know exactly want complete control fitting process. add intercept standardize design matrix critical steps data analysis. However, direct API provided use situations lasso fitting process internal component complicated algorithm standardization must handled externally.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","text":"","code":"biglasso_path(   X,   y,   r,   init = rep(0, ncol(X)),   xtx,   penalty = \"lasso\",   lambda,   alpha = 1,   gamma,   ncores = 1,   max.iter = 1000,   eps = 1e-05,   dfmax = ncol(X) + 1,   penalty.factor = rep(1, ncol(X)),   warn = TRUE,   output.time = FALSE,   return.time = TRUE )"},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","text":"X design matrix, without intercept. must double type bigmemory::big.matrix() object. y response vector r Residuals (length n vector) corresponding init. WARNING: supply incorrect value r, solution incorrect. init Initial values beta.  Default: zero (length p vector) xtx X scales: jth element equal crossprod(X[,j])/n. particular, X standardized, one pass xtx = rep(1, p). WARNING: supply incorrect value xtx, solution incorrect. (length p vector) penalty String specifying penalty use. Default 'lasso', options 'SCAD' 'MCP' (latter non-convex) lambda vector numeric values lasso tuning parameter. alpha elastic-net mixing parameter controls relative contribution lasso (l1) ridge (l2) penalty. penalty defined : $$ \\alpha||\\beta||_1 + (1-\\alpha)/2||\\beta||_2^2.$$ alpha=1 lasso penalty, alpha=0 ridge penalty, alpha 0 1 elastic-net (\"enet\") penalty. gamma Tuning parameter value nonconvex penalty. Defaults 3.7 penalty = 'SCAD' 3 penalty = 'MCP' ncores number OpenMP threads used parallel computing. max.iter Maximum number iterations.  Default 1000. eps Convergence threshold inner coordinate descent. algorithm iterates maximum change objective coefficient update less eps times null deviance. Default value 1e-7. dfmax Upper bound number nonzero coefficients. Default upper bound.  However, large data sets, computational burden may heavy models large number nonzero coefficients. penalty.factor multiplicative factor penalty applied coefficient. supplied, penalty.factor must numeric vector length equal number columns X. warn Return warning messages failures converge model saturation?  Default TRUE. output.time Whether print start end time model fitting. Default FALSE. return.time Whether return computing time model fitting. Default TRUE.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","text":"object S3 class \"biglasso\" following variables. beta sparse matrix rows estimates given coefficient across values lambda iter vector length nlambda containing number iterations convergence resid Vector residuals calculated estimated coefficients. lambda sequence regularization parameter values path. alpha biglasso() loss vector containing either residual sum squares fitted model value lambda. penalty.factor biglasso(). n number observations used model fitting. y response vector used model fitting.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_path.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","text":"biglasso_path() works identically biglasso_fit() except offers additional option fitting models across path tuning parameter values. Note: Hybrid safe-strong rules turned biglasso_fit(), rely standardization Currently, function works linear regression (family = 'gaussian').","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_path.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","text":"Tabitha Peter Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/biglasso_path.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Direct interface to biglasso fitting, no preprocessing, path version — biglasso_path","text":"","code":"data(Prostate) X <- cbind(1, Prostate$X) xtx <- apply(X, 2, crossprod)/nrow(X) y <- Prostate$y X.bm <- as.big.matrix(X) init <- rep(0, ncol(X)) fit <- biglasso_path(X = X.bm, y = y, r = y, init = init, xtx = xtx,   lambda = c(0.5, 0.1, 0.05, 0.01, 0.001),    penalty.factor=c(0, rep(1, ncol(X)-1)), max.iter=2000) fit$beta #> 9 x 5 sparse Matrix of class \"dgCMatrix\" #>                                                                       #>         1.86789798  1.726547654  1.105241004  0.62236552  0.237779016 #> lcavol  0.22521668  0.577920344  0.559394365  0.55854063  0.563967682 #> lweight .           0.042348062  0.334244868  0.55897485  0.615294573 #> age     .          -0.005532399 -0.012370678 -0.01887351 -0.020994791 #> lbph    .           0.076396177  0.083246962  0.09416931  0.096496944 #> svi     .           .            0.255311017  0.63017098  0.748190547 #> lcp     .           .            .           -0.06349802 -0.101736983 #> gleason .           .            .            .           0.042445101 #> pgg45   0.01256831  0.006710624  0.005389947  0.00498825  0.004541638    fit <- biglasso_path(X = X.bm, y = y, r = y, init = init, xtx = xtx,   lambda = c(0.5, 0.1, 0.05, 0.01, 0.001), penalty='MCP',   penalty.factor=c(0, rep(1, ncol(X)-1)), max.iter = 2000) fit$beta #> 9 x 5 sparse Matrix of class \"dgCMatrix\" #>                                                                          #>          1.648298073 -0.115420660  0.494158018  0.187554573  0.187527451 #> lcavol   0.667634641  0.600386232  0.569551761  0.564445466  0.564444994 #> lweight  .            0.753123645  0.613981343  0.621790535  0.621791579 #> age     -0.003182706 -0.017330406 -0.020888578 -0.021242328 -0.021242354 #> lbph     .            0.007112865  0.097369534  0.096738824  0.096738703 #> svi      .            .            0.752459039  0.761518580  0.761519278 #> lcp      .            .           -0.104951529 -0.106015242 -0.106015402 #> gleason  .            .            .            0.048346251  0.048350230 #> pgg45    0.005416757  0.006411446  0.005322958  0.004471521  0.004471458"},{"path":"https://pbreheny.github.io/biglasso/reference/colon.html","id":null,"dir":"Reference","previous_headings":"","what":"Gene expression data from colon-cancer patients — colon","title":"Gene expression data from colon-cancer patients — colon","text":"data file contains gene expression data 62 samples (40 tumor samples, 22 normal samples) colon-cancer patients analyzed Affymetrix oligonucleotide Hum6000 array.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/colon.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Gene expression data from colon-cancer patients — colon","text":"list 2 variables included colon: X: 62--2000 matrix records gene expression data. Used design matrix. y: binary vector length 62 recording sample status: 1 = tumor; 0 = normal. Used response vector.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/colon.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Gene expression data from colon-cancer patients — colon","text":"raw data can found Bioconductor: https://bioconductor.org/packages/release/data/experiment/html/colonCA.html.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/colon.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gene expression data from colon-cancer patients — colon","text":"U. Alon et al. (1999): Broad patterns gene expression revealed clustering analysis tumor normal colon tissue probed oligonucleotide arrays. Proc. Natl. Acad. Sci. USA 96, 6745-6750. https://www.pnas.org/doi/abs/10.1073/pnas.96.12.6745.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/colon.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gene expression data from colon-cancer patients — colon","text":"","code":"data(colon) X <- colon$X y <- colon$y str(X) #>  num [1:62, 1:2000] 8589 9164 3826 6246 3230 ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : chr [1:62] \"t\" \"n\" \"t\" \"n\" ... #>   ..$ : chr [1:2000] \"Hsa.3004\" \"Hsa.13491\" \"Hsa.13491.1\" \"Hsa.37254\" ... dim(X) #> [1]   62 2000 X.bm <- as.big.matrix(X, backingfile = \"\") # convert to big.matrix object str(X.bm) #> Formal class 'big.matrix' [package \"bigmemory\"] with 1 slot #>   ..@ address:<externalptr>  dim(X.bm) #> [1]   62 2000"},{"path":"https://pbreheny.github.io/biglasso/reference/cv.biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for biglasso — cv.biglasso","title":"Cross-validation for biglasso — cv.biglasso","text":"Perform k-fold cross validation penalized regression models grid values regularization parameter lambda.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/cv.biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for biglasso — cv.biglasso","text":"","code":"cv.biglasso(   X,   y,   row.idx = 1:nrow(X),   family = c(\"gaussian\", \"binomial\", \"cox\", \"mgaussian\"),   eval.metric = c(\"default\", \"MAPE\", \"auc\", \"class\"),   ncores = parallel::detectCores(),   ...,   nfolds = 5,   seed,   cv.ind,   trace = FALSE,   grouped = TRUE )"},{"path":"https://pbreheny.github.io/biglasso/reference/cv.biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for biglasso — cv.biglasso","text":"X design matrix, without intercept, biglasso(). y response vector, biglasso. row.idx integer vector row indices X used fitting model. biglasso. family Either \"gaussian\", \"binomial\", \"cox\" \"mgaussian\" depending response. \"cox\" \"mgaussian\" supported yet. eval.metric evaluation metric cross-validated error choosing optimal lambda. \"default\" linear regression MSE (mean squared error), logistic regression binomial deviance. \"MAPE\", linear regression , Mean Absolute Percentage Error. \"auc\", binary classification, area receiver operating characteristic curve (ROC). \"class\", binary classification, gives misclassification error. ncores number cores use parallel execution cross-validation folds, run cluster created parallel package. (also supplied ncores argument biglasso(), number OpenMP threads, first call biglasso()  run entire data. individual calls biglasso() CV folds run without ncores argument.) ... Additional arguments biglasso. nfolds number cross-validation folds.  Default 5. seed seed random number generator order obtain reproducible results. cv.ind fold observation belongs .  default observations randomly assigned cv.biglasso. trace set TRUE, cv.biglasso inform user progress announcing beginning CV fold.  Default FALSE. grouped Whether calculate CV standard error (cvse) CV folds (TRUE), cross-validated predictions. Ignored eval.metric 'auc'.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/cv.biglasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for biglasso — cv.biglasso","text":"object S3 class \"cv.biglasso\" inherits class \"cv.ncvreg\". following variables contained class (adopted ncvreg::cv.ncvreg()). cve error value lambda, averaged across cross-validation folds. cvse estimated standard error associated value cve. lambda sequence regularization parameter values along cross-validation error calculated. fit fitted biglasso object whole data. min index lambda corresponding lambda.min. lambda.min value lambda minimum cross-validation error. lambda.1se largest value lambda cross-validation error one standard error larger minimum cross-validation error. null.dev deviance intercept-model. pe family=\"binomial\", cross-validation prediction error value lambda. cv.ind .","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/cv.biglasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for biglasso — cv.biglasso","text":"function calls biglasso nfolds times, time leaving 1/nfolds data. cross-validation error based residual sum squares family=\"gaussian\" binomial deviance family=\"binomial\". S3 class object cv.biglasso inherits class ncvreg::cv.ncvreg(). S3 functions \"summary\", \"plot\" can directly applied cv.biglasso object.","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/cv.biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cross-validation for biglasso — cv.biglasso","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/cv.biglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for biglasso — cv.biglasso","text":"","code":"if (FALSE) { # \\dontrun{ ## cv.biglasso data(colon) X <- colon$X y <- colon$y X.bm <- as.big.matrix(X)  ## logistic regression cvfit <- cv.biglasso(X.bm, y, family = 'binomial', seed = 1234, ncores = 2) par(mfrow = c(2, 2)) plot(cvfit, type = 'all') summary(cvfit) } # }"},{"path":"https://pbreheny.github.io/biglasso/reference/loss.biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal biglasso functions — loss.biglasso","title":"Internal biglasso functions — loss.biglasso","text":"Internal biglasso functions","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/loss.biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal biglasso functions — loss.biglasso","text":"","code":"loss.biglasso(y, yhat, family, eval.metric, grouped = TRUE)"},{"path":"https://pbreheny.github.io/biglasso/reference/loss.biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal biglasso functions — loss.biglasso","text":"y observed response vector. yhat predicted response vector. family Either \"gaussian\" \"binomial\", depending response. eval.metric evaluation metric cross-validated error choosing optimal lambda. \"default\" linear regression MSE (mean squared error), logistic regression misclassification error. \"MAPE\", linear regression , Mean Absolute Percentage Error. \"auc\", logistic regression, area receiver operating characteristic curve (ROC). grouped Whether calculate loss entire CV fold (TRUE), predictions individually. Must TRUE eval.metric 'auc'.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/loss.biglasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal biglasso functions — loss.biglasso","text":"intended use users. loss.biglasso calculates value loss function given predictions (used cross-validation).","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/loss.biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Internal biglasso functions — loss.biglasso","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot coefficients from a ","title":"Plot coefficients from a ","text":"Produce plot coefficient paths fitted biglasso() object.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot coefficients from a ","text":"","code":"# S3 method for class 'biglasso' plot(x, alpha = 1, log.l = TRUE, ...)"},{"path":"https://pbreheny.github.io/biglasso/reference/plot.biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot coefficients from a ","text":"x Fitted biglasso() model. alpha Controls alpha-blending, helpful number covariates large.  Default alpha=1. log.l horizontal axis log scale?  Default TRUE. ... graphical parameters plot()","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/plot.biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot coefficients from a ","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.biglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot coefficients from a ","text":"","code":"## See examples in \"biglasso\""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.cv.biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots the cross-validation curve from a ","title":"Plots the cross-validation curve from a ","text":"Plot cross-validation curve cv.biglasso() object, along standard error bars.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.cv.biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots the cross-validation curve from a ","text":"","code":"# S3 method for class 'cv.biglasso' plot(   x,   log.l = TRUE,   type = c(\"cve\", \"rsq\", \"scale\", \"snr\", \"pred\", \"all\"),   selected = TRUE,   vertical.line = TRUE,   col = \"red\",   ... )"},{"path":"https://pbreheny.github.io/biglasso/reference/plot.cv.biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots the cross-validation curve from a ","text":"x \"cv.biglasso\" object. log.l horizontal axis log scale?  Default TRUE. type plot vertical axis.  cve plots cross-validation error (deviance); rsq plots estimate fraction deviance explained model (R-squared); snr plots estimate signal--noise ratio; scale plots, family=\"gaussian\", estimate scale parameter (standard deviation); pred plots, family=\"binomial\", estimated prediction error; produces . selected TRUE (default), places axis top plot denoting number variables model (.e., nonzero regression coefficient) value lambda. vertical.line TRUE (default), draws vertical line value cross-validaton error minimized. col Controls color dots (CV estimates). ... graphical parameters plot","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.cv.biglasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plots the cross-validation curve from a ","text":"Error bars representing approximate 68\\ along estimates value lambda.  rsq snr, confidence intervals quite crude, especially near.","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/plot.cv.biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plots the cross-validation curve from a ","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.cv.biglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plots the cross-validation curve from a ","text":"","code":"## See examples in \"cv.biglasso\""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.mbiglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot coefficients from a ","title":"Plot coefficients from a ","text":"Produce plot coefficient paths fitted multiple responses mbiglasso object.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.mbiglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot coefficients from a ","text":"","code":"# S3 method for class 'mbiglasso' plot(x, alpha = 1, log.l = TRUE, norm.beta = TRUE, ...)"},{"path":"https://pbreheny.github.io/biglasso/reference/plot.mbiglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot coefficients from a ","text":"x Fitted mbiglasso model. alpha Controls alpha-blending, helpful number covariates large.  Default alpha=1. log.l horizontal axis log scale?  Default TRUE. norm.beta vertical axis l2 norm coefficients variable? Default TRUE. False, vertical axis coefficients. ... graphical parameters plot()","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/plot.mbiglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot coefficients from a ","text":"Chuyi Wang","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/plot.mbiglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot coefficients from a ","text":"","code":"## See examples in \"biglasso\""},{"path":"https://pbreheny.github.io/biglasso/reference/predict.biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Model predictions based on a fitted biglasso object — predict.biglasso","title":"Model predictions based on a fitted biglasso object — predict.biglasso","text":"Extract predictions (fitted reponse, coefficients, etc.) fitted biglasso() object.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/predict.biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model predictions based on a fitted biglasso object — predict.biglasso","text":"","code":"# S3 method for class 'biglasso' predict(   object,   X,   row.idx = 1:nrow(X),   type = c(\"link\", \"response\", \"class\", \"coefficients\", \"vars\", \"nvars\"),   lambda,   which = 1:length(object$lambda),   ... )  # S3 method for class 'mbiglasso' predict(   object,   X,   row.idx = 1:nrow(X),   type = c(\"link\", \"response\", \"coefficients\", \"vars\", \"nvars\"),   lambda,   which = 1:length(object$lambda),   k = 1,   ... )  # S3 method for class 'biglasso' coef(object, lambda, which = 1:length(object$lambda), drop = TRUE, ...)  # S3 method for class 'mbiglasso' coef(object, lambda, which = 1:length(object$lambda), intercept = TRUE, ...)"},{"path":"https://pbreheny.github.io/biglasso/reference/predict.biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model predictions based on a fitted biglasso object — predict.biglasso","text":"object fitted \"biglasso\" model object. X Matrix values predictions made. must bigmemory::big.matrix() object. used type=\"coefficients\". row.idx Similar biglasso(), vector row indices X used prediction. 1:nrow(X) default. type Type prediction: \"link\" returns linear predictors \"response\" gives fitted values \"class\" returns binomial outcome highest probability \"coefficients\" returns coefficients \"vars\" returns list containing indices names nonzero variables value lambda \"nvars\" returns number nonzero coefficients value lambda lambda Values regularization parameter lambda predictions requested.  Linear interpolation used values lambda sequence lambda values fitted models. Indices penalty parameter lambda predictions required.  default, indices returned.  lambda specified, override . ... used. k Index response predict multiple responses regression ( family=\"mgaussian\"). drop coefficients single value lambda returned, reduce dimensions vector?  Setting drop=FALSE returns 1-column matrix. intercept Whether intercept included returned coefficients. family=\"mgaussian\" .","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/predict.biglasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model predictions based on a fitted biglasso object — predict.biglasso","text":"object returned depends type.","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/predict.biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Model predictions based on a fitted biglasso object — predict.biglasso","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/predict.biglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model predictions based on a fitted biglasso object — predict.biglasso","text":"","code":"## Logistic regression data(colon) X <- colon$X y <- colon$y X.bm <- as.big.matrix(X, backingfile = \"\") fit <- biglasso(X.bm, y, penalty = 'lasso', family = \"binomial\") coef <- coef(fit, lambda=0.05, drop = TRUE) coef[which(coef != 0)] #>  [1]  7.654998e-01 -5.099878e-05 -2.753690e-03 -4.182978e-04  5.594134e-05 #>  [6] -1.183261e-03  4.871641e-04  2.449309e-04  4.574918e-03 -2.180171e-04 #> [11] -1.093833e-03 -1.739847e-03  7.168045e-04  1.007572e-03 -2.752499e-03 #> [16]  4.657996e-03  5.308308e-03 predict(fit, X.bm, type=\"link\", lambda=0.05)[1:10] #>  [1]  0.8065784 -2.4540880  1.0150133 -0.5253085  2.0519542 -0.7598931 #>  [7]  2.2006058 -2.1774748  3.1595848 -2.7014291 predict(fit, X.bm, type=\"response\", lambda=0.05)[1:10] #>  [1] 0.69137991 0.07914011 0.73400010 0.37161178 0.88614493 0.31866947 #>  [7] 0.90030390 0.10179158 0.95928473 0.06288908 predict(fit, X.bm, type=\"class\", lambda=0.1)[1:10] #>  [1] 1 0 1 1 1 0 1 0 1 0 predict(fit, type=\"vars\", lambda=c(0.05, 0.1)) #> $`0.05` #>  Hsa.8147 Hsa.36689 Hsa.42949 Hsa.22762 Hsa.692.2 Hsa.31801  Hsa.3016  Hsa.5392  #>       249       377       617       639       765      1024      1325      1346  #>  Hsa.1832 Hsa.12241 Hsa.44244  Hsa.2928 Hsa.41159 Hsa.33268  Hsa.6814  Hsa.1660  #>      1423      1482      1504      1582      1641      1644      1772      1870  #>  #> $`0.1` #>  Hsa.8147 Hsa.36689 Hsa.37937  Hsa.3306 Hsa.692.2  Hsa.5392  Hsa.2928 Hsa.33268  #>       249       377       493       625       765      1346      1582      1644  #>  Hsa.6814  Hsa.1660  #>      1772      1870  #>  predict(fit, type=\"nvars\", lambda=c(0.05, 0.1)) #> 0.05  0.1  #>   16   10"},{"path":"https://pbreheny.github.io/biglasso/reference/predict.cv.biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Model predictions based on a fitted cv.biglasso() object — predict.cv.biglasso","title":"Model predictions based on a fitted cv.biglasso() object — predict.cv.biglasso","text":"Extract predictions fitted cv.biglasso() object.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/predict.cv.biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model predictions based on a fitted cv.biglasso() object — predict.cv.biglasso","text":"","code":"# S3 method for class 'cv.biglasso' predict(   object,   X,   row.idx = 1:nrow(X),   type = c(\"link\", \"response\", \"class\", \"coefficients\", \"vars\", \"nvars\"),   lambda = object$lambda.min,   which = object$min,   ... )  # S3 method for class 'cv.biglasso' coef(object, lambda = object$lambda.min, which = object$min, ...)"},{"path":"https://pbreheny.github.io/biglasso/reference/predict.cv.biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model predictions based on a fitted cv.biglasso() object — predict.cv.biglasso","text":"object fitted \"cv.biglasso\" model object. X Matrix values predictions made. must bigmemory::big.matrix() object. used type=\"coefficients\". row.idx Similar biglasso(), vector row indices X used prediction. 1:nrow(X) default. type Type prediction: \"link\" returns linear predictors \"response\" gives fitted values \"class\" returns binomial outcome highest probability \"coefficients\" returns coefficients \"vars\" returns list containing indices names nonzero variables value lambda \"nvars\" returns number nonzero coefficients value lambda lambda Values regularization parameter lambda predictions requested.  default value one corresponding minimum cross-validation error. Accepted values also strings \"lambda.min\" (lambda minimum cross-validation error) \"lambda.1se\" (Largest value lambda cross-validation error one standard error larger minimum.). Indices penalty parameter lambda predictions requested. default value index lambda corresponding lambda.min.  Note: overridden lambda specified. ... used.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/predict.cv.biglasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model predictions based on a fitted cv.biglasso() object — predict.cv.biglasso","text":"object returned depends type.","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/predict.cv.biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Model predictions based on a fitted cv.biglasso() object — predict.cv.biglasso","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/predict.cv.biglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model predictions based on a fitted cv.biglasso() object — predict.cv.biglasso","text":"","code":"if (FALSE) { # \\dontrun{ ## predict.cv.biglasso data(colon) X <- colon$X y <- colon$y X.bm <- as.big.matrix(X, backingfile = \"\") fit <- biglasso(X.bm, y, penalty = 'lasso', family = \"binomial\") cvfit <- cv.biglasso(X.bm, y, penalty = 'lasso', family = \"binomial\", seed = 1234, ncores = 2) coef <- coef(cvfit) coef[which(coef != 0)] predict(cvfit, X.bm, type = \"response\") predict(cvfit, X.bm, type = \"link\") predict(cvfit, X.bm, type = \"class\") predict(cvfit, X.bm, lambda = \"lambda.1se\") } # }"},{"path":"https://pbreheny.github.io/biglasso/reference/setupX.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up design matrix X by reading data from big data file — setupX","title":"Set up design matrix X by reading data from big data file — setupX","text":"Set design matrix X big.matrix object based external massive data file stored disk fullly loaded memory. data file must well-formated ASCII-file, contains one single type. Current version supports double type. restrictions data file described biglasso-package. function reads massive data, creates big.matrix object. default, resulting big.matrix file-backed, can shared across processors nodes cluster.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/setupX.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up design matrix X by reading data from big data file — setupX","text":"","code":"setupX(   filename,   dir = getwd(),   sep = \",\",   backingfile = paste0(unlist(strsplit(filename, split = \"\\\\.\"))[1], \".bin\"),   descriptorfile = paste0(unlist(strsplit(filename, split = \"\\\\.\"))[1], \".desc\"),   type = \"double\",   ... )"},{"path":"https://pbreheny.github.io/biglasso/reference/setupX.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up design matrix X by reading data from big data file — setupX","text":"filename name data file. example, \"dat.txt\". dir directory used store binary descriptor files associated big.matrix. default current working directory. sep field separator character. example, \",\" comma-delimited files (default); \"\\t\" tab-delimited files. backingfile binary file associated file-backed big.matrix. default, name filename extension replaced \".bin\". descriptorfile descriptor file used description file-backed big.matrix. default, name filename extension replaced \".desc\". type data type. \"double\" supported now. ... Additional arguments can passed function bigmemory::read.big.matrix().","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/setupX.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up design matrix X by reading data from big data file — setupX","text":"big.matrix object corresponding file-backed bigmemory::big.matrix(). ready used design matrix X biglasso() cv.biglasso().","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/setupX.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set up design matrix X by reading data from big data file — setupX","text":"data set, function needs called one time set big.matrix object two backing files (.bin, .desc) created current working directory. set , data can \"loaded\" (new) R session calling attach.big.matrix(discriptorfile). function simple wrapper bigmemory::read.big.matrix(). See bigmemory details.","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/setupX.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Set up design matrix X by reading data from big data file — setupX","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/setupX.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set up design matrix X by reading data from big data file — setupX","text":"","code":"## see the example in \"biglasso-package\""},{"path":"https://pbreheny.github.io/biglasso/reference/summary.cv.biglasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizing inferences based on cross-validation — summary.cv.biglasso","title":"Summarizing inferences based on cross-validation — summary.cv.biglasso","text":"Summary method cv.biglasso objects.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/summary.cv.biglasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizing inferences based on cross-validation — summary.cv.biglasso","text":"","code":"# S3 method for class 'cv.biglasso' summary(object, ...)  # S3 method for class 'summary.cv.biglasso' print(x, digits, ...)"},{"path":"https://pbreheny.github.io/biglasso/reference/summary.cv.biglasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizing inferences based on cross-validation — summary.cv.biglasso","text":"object cv.biglasso object. ... arguments passed methods. x \"summary.cv.biglasso\" object. digits Number digits past decimal point print .  Can vector specifying different display digits five non-integer printed values.","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/summary.cv.biglasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarizing inferences based on cross-validation — summary.cv.biglasso","text":"summary.cv.biglasso produces object S3 class \"summary.cv.biglasso\". class print method contains following list elements: penalty penalty used biglasso. model Either \"linear\" \"logistic\", depending family option biglasso. n Number observations p Number regression coefficients (including intercept). min index lambda smallest cross-validation error. lambda sequence lambda values used cv.biglasso. cve Cross-validation error (deviance). r.squared Proportion variance explained model, estimated cross-validation. snr Signal noise ratio, estimated cross-validation. sigma linear regression models, scale parameter estimate. pe logistic regression models, prediction error (misclassification error).","code":""},{"path":[]},{"path":"https://pbreheny.github.io/biglasso/reference/summary.cv.biglasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summarizing inferences based on cross-validation — summary.cv.biglasso","text":"Yaohui Zeng Patrick Breheny","code":""},{"path":"https://pbreheny.github.io/biglasso/reference/summary.cv.biglasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarizing inferences based on cross-validation — summary.cv.biglasso","text":"","code":"## See examples in \"cv.biglasso\" and \"biglasso-package\""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-161","dir":"Changelog","previous_headings":"","what":"biglasso 1.6.1","title":"biglasso 1.6.1","text":"Various internal fixes (see ) Updating references Fixing broken links Removing OMP directive causing stack imbalance issues Improved CI testing Eliminating use PROTECT cpp code NAMESPACE changes","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-160","dir":"Changelog","previous_headings":"","what":"biglasso 1.6.0","title":"biglasso 1.6.0","text":"CRAN release: 2024-04-21 New: functions biglasso_fit() biglasso_path(), allow users turn standardization intercept","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-152","dir":"Changelog","previous_headings":"","what":"biglasso 1.5.2","title":"biglasso 1.5.2","text":"CRAN release: 2022-10-05 Update coercion compatibility Matrix 1.5 Now using GitHub Actions instead Travis CI","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-151","dir":"Changelog","previous_headings":"","what":"biglasso 1.5.1","title":"biglasso 1.5.1","text":"CRAN release: 2022-03-09 Internal Cpp changes: initialize Xty, remove unused cutoff variable (#48) Eliminate CV test ncvreg (two packages longer use approach (#47)","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-150","dir":"Changelog","previous_headings":"","what":"biglasso 1.5.0","title":"biglasso 1.5.0","text":"CRAN release: 2022-02-08 Update headers maintain compatibility new version Rcpp (#40)","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-14-1","dir":"Changelog","previous_headings":"","what":"biglasso 1.4-1","title":"biglasso 1.4-1","text":"changed R package maintainer Chuyi Wang (wwaa0208@gmail.com) fixed bugs Add ‘auc’, ‘class’ options cv.biglasso eval.metric predict.cv now predicts standard error CV folds default; set ‘grouped’ argument FALSE old behaviour. predict.cv.biglasso accepts ‘lambda.min’, ‘lambda.1se’ argument, similar predict.cv.glmnet()","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-14-0","dir":"Changelog","previous_headings":"","what":"biglasso 1.4-0","title":"biglasso 1.4-0","text":"adaptive screening methods implemented set default applicable added sparse Cox regression removed uncompetitive screening methods combined naming screening methods version 1.4-0 CRAN submission","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-7","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-7","title":"biglasso 1.3-7","text":"CRAN release: 2019-09-09 update email personal email coef(cvfit) returns nonzero cells, labelled vector set HSR rules default option non-standardization","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-6","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-6","title":"biglasso 1.3-6","text":"CRAN release: 2017-05-04 optimized code computing slores rule. added Slores screening without active cycling (-NAC) logistic regression, research usage . corrected BEDPP elastic net. fixed bug related “exporting SSR-BEDPP”.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-5","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-5","title":"biglasso 1.3-5","text":"CRAN release: 2017-04-04 redocumented using Roxygen2. registered native routines faster stable performance.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-4","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-4","title":"biglasso 1.3-4","text":"fixed bug related dfmax option. (thanks Florian Privé!)","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-3","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-3","title":"biglasso 1.3-3","text":"CRAN release: 2017-01-26 fixed bugs related KKT checking elastic net. (thanks Florian Privé!) added references screening rules technical paper biglasso package.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-2","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-2","title":"biglasso 1.3-2","text":"added screening methods without active cycling (-NAC) comparison, research usage . fixed bug related numeric comparison Dome test.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-1","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-1","title":"biglasso 1.3-1","text":"CRAN release: 2016-12-31 fixed bug SSR-Slores related numeric equality comparison.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-13-0","dir":"Changelog","previous_headings":"","what":"biglasso 1.3-0","title":"biglasso 1.3-0","text":"CRAN release: 2016-12-21 version 1.3-0 CRAN submission.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-12-6","dir":"Changelog","previous_headings":"","what":"biglasso 1.2-6","title":"biglasso 1.2-6","text":"added newly proposed screening rule, SSR-Slores, lasso-penalized logistic regression. added SSR-BEDPP elastic-net-penalized linear regression.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-12-5","dir":"Changelog","previous_headings":"","what":"biglasso 1.2-5","title":"biglasso 1.2-5","text":"updated README.md benchmarking results. added tutorial (vignette).","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-12-4","dir":"Changelog","previous_headings":"","what":"biglasso 1.2-4","title":"biglasso 1.2-4","text":"added gaussian.cpp: solve lasso without screening, research . added tests.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-12-3","dir":"Changelog","previous_headings":"","what":"biglasso 1.2-3","title":"biglasso 1.2-3","text":"CRAN release: 2016-11-14 changed convergence criteria logistic regression glmnet. optimized source code; preparing CRAN submission. fixed memory leaks occurred Windows.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-12-2","dir":"Changelog","previous_headings":"","what":"biglasso 1.2-2","title":"biglasso 1.2-2","text":"added internal data set: colon cancer data.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-12-1","dir":"Changelog","previous_headings":"","what":"biglasso 1.2-1","title":"biglasso 1.2-1","text":"Implemented another new screening rule (SSR-BEDPP), also combining hybrid strong rule safe rule (BEDPP). implemented EDPP rule active set cycling strategy linear regression. changed convergence criteria glmnet.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-11-2","dir":"Changelog","previous_headings":"","what":"biglasso 1.1-2","title":"biglasso 1.1-2","text":"fixed bugs occurred features identical values different observations. features internally removed model fitting.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-11-1","dir":"Changelog","previous_headings":"","what":"biglasso 1.1-1","title":"biglasso 1.1-1","text":"Three sparse screening rules (SSR, EDPP, SSR-Dome) implemented. new proposed HSR-Dome combines HSR Dome test feature screening, leading even better performance compared ‘glmnet’. OpenMP parallel computing added speedup single model fitting. exact Newton majorization-minimization (MM) algorithm logistic regression implemented. latter faster, especially data-larger--RAM cases. Source code rewritten pure cpp. Sparse matrix representation added using Armadillo library.","code":""},{"path":"https://pbreheny.github.io/biglasso/news/index.html","id":"biglasso-10-1","dir":"Changelog","previous_headings":"","what":"biglasso 1.0-1","title":"biglasso 1.0-1","text":"CRAN release: 2016-03-02 package ready CRAN submission.","code":""}]
